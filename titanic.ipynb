{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The sinking of the Titanic is one of the most famous shipwrecks in history. \n",
    "\n",
    "This project aims to get us students exposed to how is Big Data is used on the real world. We are going to create a ML model to predict survival based on a Titanic passenger list dataset. One of the goals is to reach a metric goal of 0.93 AUC.\n",
    "\n",
    "We will approach the project by following instructions:\n",
    "\n",
    "- Data loading and exploration with DSS.\n",
    "- Data Preparation and cleansing.\n",
    "- Feature engineering.\n",
    "- Model creation.\n",
    "- Hyperparameter tuning to refine the model and achieve > 0.93. \n",
    "- Final model testing and interpretation of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To prepare the data we first changed the format of the training and test data to parquet. We then made a DSS recipe to clean the training parquet data and filled the empty cells from the Age column with the median age median.\n",
    "\n",
    "\n",
    "![Recipe](recipe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "After cleansing the data we started to make Visual Analysis in DSS. We started by running all the models that are better suited to make binary classification.\n",
    "\n",
    "Below is an image of how the models behaved on the first run:\n",
    "\n",
    "![Models](models.png)\n",
    "\n",
    "We decided to go foward with the top 4 scoring models and train until a > 0.93 ROC AUC score:\n",
    "1. XGBoost\n",
    "2. Gradient Tree Boosting\n",
    "3. Decision Trees\n",
    "4. Support Vector Machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this phase, we manipulated the dataset to refine the features for our predictive analysis. We made several strategic decisions to enhance model performance:\n",
    "\n",
    "- The **Cabin** column, with 77.0% of its rows missing, was omitted from our feature set to avoid introducing bias due to the large amount of imputation that would be required.\n",
    "- We changed the variable type of `PClass` to a categorical variable rather than numerical, as its ordinal nature has more in common with categorical data.\n",
    "- Through iterative training and performance evaluation, we made informed choices about feature inclusion. For instance, we critically evaluated the role of **Parch** – the number of parents and children aboard – to determine its impact on our model.\n",
    "\n",
    "Below is an illustration of the features after our engineering steps:\n",
    "\n",
    "![Features](features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Hyperparameter Tuning\n",
    "\n",
    "With our top model candidates identified and features optimized, we proceeded to the training phase.\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "We began by training the **XGBoost** model, which initially yielded the highest score. XGBoost is an optimized distributed gradient boosting library renowned for its efficiency, flexibility, and portability. It implements machine learning algorithms under the Gradient Boosting framework. [1]\n",
    "\n",
    "#### Hyperparameters Tuned for XGBoost:\n",
    "- **Number of Trees**: We experimented with the number of trees, finding that a range from 100 to 125 trees yielded a score of 0.916 ROC AUC.\n",
    "- **Maximum Depth of a Tree**: Adjusting the max depth of trees showed that with a depth of 4 and 100 trees, we achieved a score of 0.917.\n",
    "- **Learning Rate**: Initially set at 0.2, adjusting the learning rate to 0.215 improved our score to 0.927 ROC AUC.\n",
    "\n",
    "After iteratively revisiting and fine-tuning these parameters, we achieved our highest score of 0.929 ROC AUC using 107 trees, a max depth of 4, and a learning rate of 0.215.\n",
    "\n",
    "#### Highest Score with XGBoost:\n",
    "\n",
    "![XGBoost](pictures/XGBoost.png)\n",
    "\n",
    "[1]: [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
